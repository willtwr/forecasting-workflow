{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e49ee8",
   "metadata": {},
   "source": [
    "# Healthcare No Show Modeling\n",
    "\n",
    "- Train a ML model to forecast no show\n",
    "- Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c40d67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")  # add src to environment path so that custom modules can be found\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from src.models.logistic_regression import LogisticRegressionClassifier\n",
    "from src.models.mlp import MLPClassifier\n",
    "from src.models.fttransformer import FTTransformerClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba4811",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a53f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_cross_val = 0\n",
    "full_dataset = torch.load(\n",
    "    f\"../../data/healthcare_no_show/healthcare_datasets_base_{idx_cross_val}.pt\",\n",
    "    weights_only=False\n",
    ")\n",
    "train_dataset = full_dataset[\"train_dataset\"]\n",
    "val_dataset = full_dataset[\"val_dataset\"]\n",
    "feature_sizes = full_dataset[\"feature_sizes\"]\n",
    "n_classes = full_dataset[\"class_size\"] if full_dataset[\"class_size\"] > 2 else 1\n",
    "sampler = WeightedRandomSampler(torch.DoubleTensor(full_dataset[\"sample_weights\"]), int(full_dataset[\"total_samples\"]))\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58445b6f",
   "metadata": {},
   "source": [
    "## Hyperparameters, functions, and model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb033e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tensorboard\n",
    "writer = SummaryWriter(f\"../../runs/healthcare_no_show_data{idx_cross_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d3bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations that don't change with experiments\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d65786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ced3c2ebb30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset random seed\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionClassifier(\n",
    "    feature_cats=feature_sizes,\n",
    "    num_classes=n_classes,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af9217",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(\n",
    "    feature_cats=feature_sizes,\n",
    "    num_classes=n_classes,\n",
    "    num_hidden_neurons=3,\n",
    "    num_hidden_layers=256\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc902e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FTTransformerClassifier(\n",
    "    feature_cats=feature_sizes,\n",
    "    num_classes=n_classes,\n",
    "    d_model=256,\n",
    "    num_encoder_layers=3,\n",
    "    dim_feedforward=384,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86cea0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_set1 = []\n",
    "params_set2 = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"cls_token\" not in name and \"embeddings\" not in name and \"bias\" not in name and \"norm\" not in name:\n",
    "        params_set1.append(param)\n",
    "    else:\n",
    "        params_set2.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used by FT-Transformer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c64c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate optimizer for bias, norm and inputs with weight decay = 0\n",
    "optimizer1 = torch.optim.AdamW(params_set1, lr=1e-4, weight_decay=1e-5)\n",
    "optimizer2 = torch.optim.AdamW(params_set2, lr=1e-4, weight_decay=0)\n",
    "optimizer = [optimizer1, optimizer2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schedule learning rate reduction\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9dac23",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db21d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6547, Val Loss: 0.6562\n",
      "Epoch [2/100], Loss: 0.6259, Val Loss: 0.5820\n",
      "Epoch [3/100], Loss: 0.6178, Val Loss: 0.6155\n",
      "Epoch [4/100], Loss: 0.6161, Val Loss: 0.6005\n",
      "Epoch [5/100], Loss: 0.6083, Val Loss: 0.5864\n",
      "Epoch [6/100], Loss: 0.6042, Val Loss: 0.6064\n",
      "Epoch [7/100], Loss: 0.6082, Val Loss: 0.5694\n",
      "Epoch [8/100], Loss: 0.6032, Val Loss: 0.5943\n",
      "Epoch [9/100], Loss: 0.6002, Val Loss: 0.6596\n",
      "Epoch [10/100], Loss: 0.5962, Val Loss: 0.6197\n",
      "Epoch [11/100], Loss: 0.5995, Val Loss: 0.6089\n",
      "Epoch [12/100], Loss: 0.5969, Val Loss: 0.6150\n",
      "Epoch [13/100], Loss: 0.5998, Val Loss: 0.5965\n",
      "Epoch [14/100], Loss: 0.5945, Val Loss: 0.5987\n",
      "Epoch [15/100], Loss: 0.5948, Val Loss: 0.5947\n",
      "Epoch [16/100], Loss: 0.5927, Val Loss: 0.6112\n",
      "Epoch [17/100], Loss: 0.5974, Val Loss: 0.5713\n",
      "Epoch [18/100], Loss: 0.5931, Val Loss: 0.6152\n",
      "Epoch [19/100], Loss: 0.5966, Val Loss: 0.6117\n",
      "Epoch [20/100], Loss: 0.5945, Val Loss: 0.6105\n",
      "Epoch [21/100], Loss: 0.5924, Val Loss: 0.6002\n",
      "Epoch [22/100], Loss: 0.5932, Val Loss: 0.6158\n",
      "Epoch [23/100], Loss: 0.5925, Val Loss: 0.5969\n",
      "Epoch [24/100], Loss: 0.5932, Val Loss: 0.6065\n",
      "Epoch [25/100], Loss: 0.5898, Val Loss: 0.6143\n",
      "Epoch [26/100], Loss: 0.5915, Val Loss: 0.5866\n",
      "Epoch [27/100], Loss: 0.5917, Val Loss: 0.5964\n",
      "Epoch [28/100], Loss: 0.5932, Val Loss: 0.5934\n",
      "Epoch [29/100], Loss: 0.5895, Val Loss: 0.5940\n",
      "Epoch [30/100], Loss: 0.5889, Val Loss: 0.6016\n",
      "Epoch [31/100], Loss: 0.5925, Val Loss: 0.5965\n",
      "Epoch [32/100], Loss: 0.5887, Val Loss: 0.5876\n",
      "Epoch [33/100], Loss: 0.5922, Val Loss: 0.6086\n",
      "Epoch [34/100], Loss: 0.5899, Val Loss: 0.5939\n",
      "Epoch [35/100], Loss: 0.5880, Val Loss: 0.6126\n",
      "Epoch [36/100], Loss: 0.5907, Val Loss: 0.5821\n",
      "Epoch [37/100], Loss: 0.5888, Val Loss: 0.5926\n",
      "Epoch [38/100], Loss: 0.5914, Val Loss: 0.5901\n",
      "Epoch [39/100], Loss: 0.5906, Val Loss: 0.5948\n",
      "Epoch [40/100], Loss: 0.5859, Val Loss: 0.6046\n",
      "Epoch [41/100], Loss: 0.5866, Val Loss: 0.6033\n",
      "Epoch [42/100], Loss: 0.5863, Val Loss: 0.6013\n",
      "Epoch [43/100], Loss: 0.5880, Val Loss: 0.6054\n",
      "Epoch [44/100], Loss: 0.5867, Val Loss: 0.6032\n",
      "Epoch [45/100], Loss: 0.5875, Val Loss: 0.6065\n",
      "Epoch [46/100], Loss: 0.5846, Val Loss: 0.5863\n",
      "Epoch [47/100], Loss: 0.5882, Val Loss: 0.6072\n",
      "Epoch [48/100], Loss: 0.5864, Val Loss: 0.6052\n",
      "Epoch [49/100], Loss: 0.5835, Val Loss: 0.6009\n",
      "Epoch [50/100], Loss: 0.5860, Val Loss: 0.5871\n",
      "Epoch [51/100], Loss: 0.5856, Val Loss: 0.5928\n",
      "Epoch [52/100], Loss: 0.5860, Val Loss: 0.5745\n",
      "Epoch [53/100], Loss: 0.5843, Val Loss: 0.5932\n",
      "Epoch [54/100], Loss: 0.5880, Val Loss: 0.5806\n",
      "Epoch [55/100], Loss: 0.5839, Val Loss: 0.6007\n",
      "Epoch [56/100], Loss: 0.5863, Val Loss: 0.6008\n",
      "Epoch [57/100], Loss: 0.5843, Val Loss: 0.6214\n",
      "Epoch [58/100], Loss: 0.5850, Val Loss: 0.5863\n",
      "Epoch [59/100], Loss: 0.5862, Val Loss: 0.6130\n",
      "Epoch [60/100], Loss: 0.5816, Val Loss: 0.6066\n",
      "Epoch [61/100], Loss: 0.5809, Val Loss: 0.6258\n",
      "Epoch [62/100], Loss: 0.5809, Val Loss: 0.6030\n",
      "Epoch [63/100], Loss: 0.5831, Val Loss: 0.6056\n",
      "Epoch [64/100], Loss: 0.5824, Val Loss: 0.6314\n",
      "Epoch [65/100], Loss: 0.5799, Val Loss: 0.6192\n",
      "Epoch [66/100], Loss: 0.5831, Val Loss: 0.6167\n",
      "Epoch [67/100], Loss: 0.5813, Val Loss: 0.6204\n",
      "Epoch [68/100], Loss: 0.5837, Val Loss: 0.5992\n",
      "Epoch [69/100], Loss: 0.5802, Val Loss: 0.6132\n",
      "Epoch [70/100], Loss: 0.5815, Val Loss: 0.6080\n",
      "Epoch [71/100], Loss: 0.5799, Val Loss: 0.6009\n",
      "Epoch [72/100], Loss: 0.5806, Val Loss: 0.6049\n",
      "Epoch [73/100], Loss: 0.5839, Val Loss: 0.5961\n",
      "Epoch [74/100], Loss: 0.5796, Val Loss: 0.6183\n",
      "Epoch [75/100], Loss: 0.5808, Val Loss: 0.5985\n",
      "Epoch [76/100], Loss: 0.5789, Val Loss: 0.6144\n",
      "Epoch [77/100], Loss: 0.5805, Val Loss: 0.6352\n",
      "Epoch [78/100], Loss: 0.5779, Val Loss: 0.6129\n",
      "Epoch [79/100], Loss: 0.5798, Val Loss: 0.6018\n",
      "Epoch [80/100], Loss: 0.5801, Val Loss: 0.6023\n",
      "Epoch [81/100], Loss: 0.5779, Val Loss: 0.6186\n",
      "Epoch [82/100], Loss: 0.5778, Val Loss: 0.6206\n",
      "Epoch [83/100], Loss: 0.5801, Val Loss: 0.6116\n",
      "Epoch [84/100], Loss: 0.5828, Val Loss: 0.6086\n",
      "Epoch [85/100], Loss: 0.5777, Val Loss: 0.6028\n",
      "Epoch [86/100], Loss: 0.5738, Val Loss: 0.6131\n",
      "Epoch [87/100], Loss: 0.5764, Val Loss: 0.6153\n",
      "Epoch [88/100], Loss: 0.5801, Val Loss: 0.6246\n",
      "Epoch [89/100], Loss: 0.5747, Val Loss: 0.6174\n",
      "Epoch [90/100], Loss: 0.5767, Val Loss: 0.6154\n",
      "Epoch [91/100], Loss: 0.5756, Val Loss: 0.6064\n",
      "Epoch [92/100], Loss: 0.5741, Val Loss: 0.6086\n",
      "Epoch [93/100], Loss: 0.5751, Val Loss: 0.6164\n",
      "Epoch [94/100], Loss: 0.5785, Val Loss: 0.5999\n",
      "Epoch [95/100], Loss: 0.5744, Val Loss: 0.5943\n",
      "Epoch [96/100], Loss: 0.5761, Val Loss: 0.6201\n",
      "Epoch [97/100], Loss: 0.5748, Val Loss: 0.6176\n",
      "Epoch [98/100], Loss: 0.5778, Val Loss: 0.6260\n",
      "Epoch [99/100], Loss: 0.5736, Val Loss: 0.6040\n",
      "Epoch [100/100], Loss: 0.5756, Val Loss: 0.6289\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for iter_idx, (features, labels) in enumerate(train_loader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        if isinstance(optimizer, list):\n",
    "            [x.zero_grad() for x in optimizer]\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "\n",
    "        if isinstance(optimizer, list):\n",
    "            [x.step() for x in optimizer]\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if \"writer\" in globals():\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), epoch * len(train_loader) + iter_idx)\n",
    "    \n",
    "    if \"scheduler\" in globals():\n",
    "        scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for features, labels in val_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            val_loss += criterion(outputs, labels.unsqueeze(1)).item()\n",
    "\n",
    "        if \"writer\" in globals():\n",
    "            writer.add_scalar(\"Loss/val\", val_loss / len(val_loader), epoch)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95369f95",
   "metadata": {},
   "source": [
    "## Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68579d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7948\n",
      "Precision: 0.9946201071156904\n",
      "Recall: 0.7947975571137752\n",
      "F-score: 0.8824003890680735\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.9\n",
    "preds = []\n",
    "trues = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in val_loader:\n",
    "        features = features.to(device)\n",
    "        outputs = model(features)\n",
    "        predictions = torch.sigmoid(outputs.squeeze()).cpu().numpy()\n",
    "        preds.append(predictions)\n",
    "        trues.append(labels.numpy())\n",
    "\n",
    "preds = np.concat(preds, axis=0) > threshold\n",
    "trues = np.concat(trues, axis=0)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(preds, trues, average='weighted')\n",
    "print(f\"Validation Accuracy: {(preds == trues).mean():.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F-score: {fscore:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa4a2d",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* acc1 refers to accuracy of dataset 1\n",
    "\n",
    "Results of MLP:\n",
    "\n",
    "| exp | opt type | LR | weight decay | acc0 | acc1 | acc2 | avg acc |\n",
    "|----|----|----|----|----|----|----|----|\n",
    "| ?* | SGD | 1e-2 | 1e-4 | 0.7961 | ? | ? | ? |\n",
    "| ? | SGD | 1e-3 | 1e-2 | 0.7961 | ? | ? | ? |\n",
    "| ?* | Adam | 1e-3 | 0 | ? | ? | ? | ? |\n",
    "| ?* | Adam | 1e-3 | 1e-5 | ? | ? | ? | ? |\n",
    "| ? | Adam | 1e-3 | 1e-2 | ? | ? | ? | ? |\n",
    "| ? | Adam | 1e-3 | 1e-4 | ? | ? | ? | ? |\n",
    "| ?* | AdamW | 1e-4 | 1e-2 | ? | ? | ? | ? |\n",
    "| ?* | AdamW | 1e-3 | 1e-2 | ? | ? | ? | ? |\n",
    "| ? | AdamW | 1e-3 | 1e-4 | ? | ? | ? | ? |\n",
    "\n",
    "LR: 0.7915\n",
    "\n",
    "smaller lr no scheduler: 0.7937\n",
    "\n",
    "MLP: 0.7944\n",
    "\n",
    "TFTrans: 0.7944\n",
    "\n",
    "smaller lr no scheduler: 0.7951"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c382522",
   "metadata": {},
   "source": [
    "## Store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7311f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"../../models/healthcare_no_show/transformer_classifier_data{idx_cross_val}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
