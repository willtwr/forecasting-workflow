{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e49ee8",
   "metadata": {},
   "source": [
    "# Healthcare No Show Modeling\n",
    "\n",
    "- Train a ML model to forecast no show\n",
    "- Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40d67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")  # add src to environment path so that custom modules can be found\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba4811",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a53f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_cross_val = 0\n",
    "full_dataset = torch.load(\n",
    "    f\"../../data/healthcare_no_show/healthcare_datasets_{idx_cross_val}.pt\",\n",
    "    weights_only=False\n",
    ")\n",
    "train_dataset = full_dataset[\"train_dataset\"]\n",
    "val_dataset = full_dataset[\"val_dataset\"]\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60320cd3",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"A transformer-based classifier for sequence data.\n",
    "    \n",
    "    This model uses a transformer encoder architecture followed by a classification layer\n",
    "    to perform sequence classification tasks.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Dimension of input features\n",
    "        num_classes (int): Number of output classes\n",
    "        d_model (int, optional): Dimension of transformer model. Defaults to 512.\n",
    "        nhead (int, optional): Number of attention heads. Defaults to 8.\n",
    "        num_encoder_layers (int, optional): Number of transformer encoder layers. Defaults to 3.\n",
    "        dim_feedforward (int, optional): Dimension of feedforward network. Defaults to 2048.\n",
    "        dropout (float, optional): Dropout rate. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim: int, \n",
    "            num_classes: int, \n",
    "            d_model: int = 512, \n",
    "            nhead: int = 8, \n",
    "            num_encoder_layers: int = 3, \n",
    "            dim_feedforward: int = 2048, \n",
    "            dropout: float = 0.1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection layer\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.bn_input = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        # Output classifier\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Project input to d_model dimensions\n",
    "        x = self.input_projection(x)\n",
    "        x = self.bn_input(x)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x.unsqueeze(1))  # Add sequence dimension\n",
    "        \n",
    "        # Classification layer\n",
    "        output = self.classifier(x.squeeze())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58445b6f",
   "metadata": {},
   "source": [
    "## Hyperparameters and functions initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38d3bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb033e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 3\n",
    "writer = SummaryWriter(f\"../../runs/healthcare_no_show_exp{exp}_data{idx_cross_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc902e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "n_features = train_dataset[0][0].shape[0]\n",
    "n_classes = 1\n",
    "model = TransformerClassifier(\n",
    "    input_dim=n_features,\n",
    "    num_classes=n_classes,\n",
    "    num_encoder_layers=3\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "158d9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "match exp:\n",
    "    case 0:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-2)  # exp0\n",
    "    case 1:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)  # exp1\n",
    "    case 2:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)  # exp2\n",
    "    case 3:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)  # exp3\n",
    "    case _:\n",
    "        raise ValueError(f\"Unknown experiment {exp}\")\n",
    "    \n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9dac23",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6db21d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.5037, Val Loss: 0.4748\n",
      "Epoch [2/100], Loss: 0.4589, Val Loss: 0.4854\n",
      "Epoch [3/100], Loss: 0.4602, Val Loss: 0.4598\n",
      "Epoch [4/100], Loss: 0.4692, Val Loss: 0.4695\n",
      "Epoch [5/100], Loss: 0.4641, Val Loss: 0.4723\n",
      "Epoch [6/100], Loss: 0.4706, Val Loss: 0.4756\n",
      "Epoch [7/100], Loss: 0.4658, Val Loss: 0.4704\n",
      "Epoch [8/100], Loss: 0.4658, Val Loss: 0.4791\n",
      "Epoch [9/100], Loss: 0.4646, Val Loss: 0.4627\n",
      "Epoch [10/100], Loss: 0.4584, Val Loss: 0.4664\n",
      "Epoch [11/100], Loss: 0.4598, Val Loss: 0.4690\n",
      "Epoch [12/100], Loss: 0.4664, Val Loss: 0.4693\n",
      "Epoch [13/100], Loss: 0.4917, Val Loss: 0.4960\n",
      "Epoch [14/100], Loss: 0.4927, Val Loss: 0.4964\n",
      "Epoch [15/100], Loss: 0.4703, Val Loss: 0.4764\n",
      "Epoch [16/100], Loss: 0.4672, Val Loss: 0.4708\n",
      "Epoch [17/100], Loss: 0.4672, Val Loss: 0.4709\n",
      "Epoch [18/100], Loss: 0.4672, Val Loss: 0.4702\n",
      "Epoch [19/100], Loss: 0.4672, Val Loss: 0.4709\n",
      "Epoch [20/100], Loss: 0.4671, Val Loss: 0.4705\n",
      "Epoch [21/100], Loss: 0.4670, Val Loss: 0.4700\n",
      "Epoch [22/100], Loss: 0.4668, Val Loss: 0.4691\n",
      "Epoch [23/100], Loss: 0.4668, Val Loss: 0.4705\n",
      "Epoch [24/100], Loss: 0.4666, Val Loss: 0.4692\n",
      "Epoch [25/100], Loss: 0.4664, Val Loss: 0.4697\n",
      "Epoch [26/100], Loss: 0.4684, Val Loss: 0.4697\n",
      "Epoch [27/100], Loss: 0.4633, Val Loss: 0.4671\n",
      "Epoch [28/100], Loss: 0.4612, Val Loss: 0.4641\n",
      "Epoch [29/100], Loss: 0.4588, Val Loss: 0.4625\n",
      "Epoch [30/100], Loss: 0.4577, Val Loss: 0.4608\n",
      "Epoch [31/100], Loss: 0.4572, Val Loss: 0.4615\n",
      "Epoch [32/100], Loss: 0.4571, Val Loss: 0.4613\n",
      "Epoch [33/100], Loss: 0.4575, Val Loss: 0.4617\n",
      "Epoch [34/100], Loss: 0.4580, Val Loss: 0.4620\n",
      "Epoch [35/100], Loss: 0.4577, Val Loss: 0.4615\n",
      "Epoch [36/100], Loss: 0.4577, Val Loss: 0.4616\n",
      "Epoch [37/100], Loss: 0.4576, Val Loss: 0.4620\n",
      "Epoch [38/100], Loss: 0.4576, Val Loss: 0.4617\n",
      "Epoch [39/100], Loss: 0.4579, Val Loss: 0.4631\n",
      "Epoch [40/100], Loss: 0.4577, Val Loss: 0.4618\n",
      "Epoch [41/100], Loss: 0.4576, Val Loss: 0.4628\n",
      "Epoch [42/100], Loss: 0.4576, Val Loss: 0.4621\n",
      "Epoch [43/100], Loss: 0.4576, Val Loss: 0.4629\n",
      "Epoch [44/100], Loss: 0.4576, Val Loss: 0.4621\n",
      "Epoch [45/100], Loss: 0.4577, Val Loss: 0.4619\n",
      "Epoch [46/100], Loss: 0.4578, Val Loss: 0.4619\n",
      "Epoch [47/100], Loss: 0.4576, Val Loss: 0.4622\n",
      "Epoch [48/100], Loss: 0.4579, Val Loss: 0.4624\n",
      "Epoch [49/100], Loss: 0.4583, Val Loss: 0.4623\n",
      "Epoch [50/100], Loss: 0.4579, Val Loss: 0.4617\n",
      "Epoch [51/100], Loss: 0.4589, Val Loss: 0.4631\n",
      "Epoch [52/100], Loss: 0.4593, Val Loss: 0.4629\n",
      "Epoch [53/100], Loss: 0.4591, Val Loss: 0.4639\n",
      "Epoch [54/100], Loss: 0.4594, Val Loss: 0.4634\n",
      "Epoch [55/100], Loss: 0.4590, Val Loss: 0.4636\n",
      "Epoch [56/100], Loss: 0.4592, Val Loss: 0.4634\n",
      "Epoch [57/100], Loss: 0.4591, Val Loss: 0.4637\n",
      "Epoch [58/100], Loss: 0.4590, Val Loss: 0.4634\n",
      "Epoch [59/100], Loss: 0.4592, Val Loss: 0.4635\n",
      "Epoch [60/100], Loss: 0.4592, Val Loss: 0.4629\n",
      "Epoch [61/100], Loss: 0.4593, Val Loss: 0.4634\n",
      "Epoch [62/100], Loss: 0.4590, Val Loss: 0.4635\n",
      "Epoch [63/100], Loss: 0.4591, Val Loss: 0.4635\n",
      "Epoch [64/100], Loss: 0.4590, Val Loss: 0.4635\n",
      "Epoch [65/100], Loss: 0.4591, Val Loss: 0.4634\n",
      "Epoch [66/100], Loss: 0.4591, Val Loss: 0.4635\n",
      "Epoch [67/100], Loss: 0.4591, Val Loss: 0.4635\n",
      "Epoch [68/100], Loss: 0.4594, Val Loss: 0.4634\n",
      "Epoch [69/100], Loss: 0.4592, Val Loss: 0.4634\n",
      "Epoch [70/100], Loss: 0.4594, Val Loss: 0.4635\n",
      "Epoch [71/100], Loss: 0.4595, Val Loss: 0.4636\n",
      "Epoch [72/100], Loss: 0.4596, Val Loss: 0.4636\n",
      "Epoch [73/100], Loss: 0.4595, Val Loss: 0.4636\n",
      "Epoch [74/100], Loss: 0.4595, Val Loss: 0.4635\n",
      "Epoch [75/100], Loss: 0.4596, Val Loss: 0.4635\n",
      "Epoch [76/100], Loss: 0.4596, Val Loss: 0.4636\n",
      "Epoch [77/100], Loss: 0.4598, Val Loss: 0.4634\n",
      "Epoch [78/100], Loss: 0.4598, Val Loss: 0.4633\n",
      "Epoch [79/100], Loss: 0.4599, Val Loss: 0.4635\n",
      "Epoch [80/100], Loss: 0.4598, Val Loss: 0.4634\n",
      "Epoch [81/100], Loss: 0.4598, Val Loss: 0.4634\n",
      "Epoch [82/100], Loss: 0.4596, Val Loss: 0.4635\n",
      "Epoch [83/100], Loss: 0.4595, Val Loss: 0.4635\n",
      "Epoch [84/100], Loss: 0.4597, Val Loss: 0.4635\n",
      "Epoch [85/100], Loss: 0.4596, Val Loss: 0.4634\n",
      "Epoch [86/100], Loss: 0.4599, Val Loss: 0.4634\n",
      "Epoch [87/100], Loss: 0.4598, Val Loss: 0.4634\n",
      "Epoch [88/100], Loss: 0.4601, Val Loss: 0.4634\n",
      "Epoch [89/100], Loss: 0.4599, Val Loss: 0.4634\n",
      "Epoch [90/100], Loss: 0.4599, Val Loss: 0.4634\n",
      "Epoch [91/100], Loss: 0.4598, Val Loss: 0.4634\n",
      "Epoch [92/100], Loss: 0.4600, Val Loss: 0.4634\n",
      "Epoch [93/100], Loss: 0.4598, Val Loss: 0.4634\n",
      "Epoch [94/100], Loss: 0.4600, Val Loss: 0.4634\n",
      "Epoch [95/100], Loss: 0.4598, Val Loss: 0.4634\n",
      "Epoch [96/100], Loss: 0.4599, Val Loss: 0.4634\n",
      "Epoch [97/100], Loss: 0.4601, Val Loss: 0.4634\n",
      "Epoch [98/100], Loss: 0.4600, Val Loss: 0.4634\n",
      "Epoch [99/100], Loss: 0.4599, Val Loss: 0.4634\n",
      "Epoch [100/100], Loss: 0.4600, Val Loss: 0.4634\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for iter_idx, (features, labels) in enumerate(train_loader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar(\"Loss/train\", loss.item(), epoch * len(train_loader) + iter_idx)\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for features, labels in val_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            val_loss += criterion(outputs, labels.unsqueeze(1)).item()\n",
    "\n",
    "        writer.add_scalar(\"Loss/val\", val_loss / len(val_loader), epoch)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95369f95",
   "metadata": {},
   "source": [
    "## Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68579d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7960\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in val_loader:\n",
    "        features = features.to(device)\n",
    "        outputs = model(features)\n",
    "        predictions = torch.sigmoid(outputs.squeeze()).cpu().numpy()\n",
    "        accuracies.append((predictions > 0.5) == labels.numpy())\n",
    "\n",
    "print(f\"Validation Accuracy: {np.concat(accuracies, axis=0).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa4a2d",
   "metadata": {},
   "source": [
    "Results (acc1 refers to accuracy of dataset 1):\n",
    "| exp | opt type | weight decay | acc0 | acc1 | acc2 | avg acc |\n",
    "|----|----|----|----|----|----|----|\n",
    "| 0 | Adam | 1e-2 | 0.7955 | ? | ? | ? |\n",
    "| 1 | Adam | 1e-4 | 0.7960 | ? | ? | ? |\n",
    "| 2 | AdamW | 1e-2 | 0.7960 | ? | ? | ? |\n",
    "| 3 | AdamW | 1e-4 | 0.7960 | ? | ? | ? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c382522",
   "metadata": {},
   "source": [
    "## Store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7311f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"../../models/healthcare_no_show/transformer_classifier_exp{exp}_data{idx_cross_val}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
