{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e49ee8",
   "metadata": {},
   "source": [
    "# Healthcare No Show Machine Learning\n",
    "\n",
    "Use machine learning technique to forecast no show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")  # add src to path to import custom modules\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba4811",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7777388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60320cd3",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "496b13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        features = torch.tensor(row[:-1].values, dtype=torch.float32)\n",
    "        label = torch.tensor(row.iloc[-1], dtype=torch.float32)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0a53f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CustomDataset(df)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ae6621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"A transformer-based classifier for sequence data.\n",
    "    \n",
    "    This model uses a transformer encoder architecture followed by a classification layer\n",
    "    to perform sequence classification tasks.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Dimension of input features\n",
    "        num_classes (int): Number of output classes\n",
    "        d_model (int, optional): Dimension of transformer model. Defaults to 512.\n",
    "        nhead (int, optional): Number of attention heads. Defaults to 8.\n",
    "        num_encoder_layers (int, optional): Number of transformer encoder layers. Defaults to 3.\n",
    "        dim_feedforward (int, optional): Dimension of feedforward network. Defaults to 2048.\n",
    "        dropout (float, optional): Dropout rate. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim: int, \n",
    "            num_classes: int, \n",
    "            d_model: int = 512, \n",
    "            nhead: int = 8, \n",
    "            num_encoder_layers: int = 3, \n",
    "            dim_feedforward: int = 2048, \n",
    "            dropout: float = 0.1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection layer\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        nn.init.xavier_normal_(self.input_projection.weight)\n",
    "        nn.init.constant_(self.input_projection.bias, 0.1)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        for name, param in self.transformer_encoder.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param.unsqueeze(0))\n",
    "        \n",
    "        # Output classifier\n",
    "        self.classifier = nn.Linear(d_model, num_classes, bias=False)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Project input to d_model dimensions\n",
    "        x = self.input_projection(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Classification layer\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc902e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_features = len(df.columns) - 1\n",
    "n_classes = 1\n",
    "model = TransformerClassifier(\n",
    "    input_dim=n_features,\n",
    "    num_classes=n_classes,\n",
    "    num_encoder_layers=6\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6db21d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_920/1738235054.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = torch.tensor(row[-1], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.5051, Val Loss: 0.4861\n",
      "Epoch [2/100], Loss: 0.4974, Val Loss: 0.4990\n",
      "Epoch [3/100], Loss: 0.5046, Val Loss: 0.4980\n",
      "Epoch [4/100], Loss: 0.4758, Val Loss: 0.4529\n",
      "Epoch [5/100], Loss: 0.4533, Val Loss: 0.4439\n",
      "Epoch [6/100], Loss: 0.4493, Val Loss: 0.4451\n",
      "Epoch [7/100], Loss: 0.4487, Val Loss: 0.4455\n",
      "Epoch [8/100], Loss: 0.4472, Val Loss: 0.4434\n",
      "Epoch [9/100], Loss: 0.4455, Val Loss: 0.4417\n",
      "Epoch [10/100], Loss: 0.4441, Val Loss: 0.4417\n",
      "Epoch [11/100], Loss: 0.4446, Val Loss: 0.4464\n",
      "Epoch [12/100], Loss: 0.4441, Val Loss: 0.4402\n",
      "Epoch [13/100], Loss: 0.4412, Val Loss: 0.4405\n",
      "Epoch [14/100], Loss: 0.4409, Val Loss: 0.4445\n",
      "Epoch [15/100], Loss: 0.4394, Val Loss: 0.4401\n",
      "Epoch [16/100], Loss: 0.4403, Val Loss: 0.4439\n",
      "Epoch [17/100], Loss: 0.4384, Val Loss: 0.4404\n",
      "Epoch [18/100], Loss: 0.4360, Val Loss: 0.4400\n",
      "Epoch [19/100], Loss: 0.4347, Val Loss: 0.4402\n",
      "Epoch [20/100], Loss: 0.4329, Val Loss: 0.4459\n",
      "Epoch [21/100], Loss: 0.4318, Val Loss: 0.4410\n",
      "Epoch [22/100], Loss: 0.4322, Val Loss: 0.4401\n",
      "Epoch [23/100], Loss: 0.4299, Val Loss: 0.4513\n",
      "Epoch [24/100], Loss: 0.4278, Val Loss: 0.4407\n",
      "Epoch [25/100], Loss: 0.4269, Val Loss: 0.4467\n",
      "Epoch [26/100], Loss: 0.4251, Val Loss: 0.4513\n",
      "Epoch [27/100], Loss: 0.4231, Val Loss: 0.4483\n",
      "Epoch [28/100], Loss: 0.4229, Val Loss: 0.4479\n",
      "Epoch [29/100], Loss: 0.4207, Val Loss: 0.4476\n",
      "Epoch [30/100], Loss: 0.4196, Val Loss: 0.4522\n",
      "Epoch [31/100], Loss: 0.4080, Val Loss: 0.4533\n",
      "Epoch [32/100], Loss: 0.4046, Val Loss: 0.4540\n",
      "Epoch [33/100], Loss: 0.4027, Val Loss: 0.4589\n",
      "Epoch [34/100], Loss: 0.4014, Val Loss: 0.4583\n",
      "Epoch [35/100], Loss: 0.3995, Val Loss: 0.4562\n",
      "Epoch [36/100], Loss: 0.3984, Val Loss: 0.4605\n",
      "Epoch [37/100], Loss: 0.3966, Val Loss: 0.4652\n",
      "Epoch [38/100], Loss: 0.3958, Val Loss: 0.4641\n",
      "Epoch [39/100], Loss: 0.3945, Val Loss: 0.4645\n",
      "Epoch [40/100], Loss: 0.3934, Val Loss: 0.4635\n",
      "Epoch [41/100], Loss: 0.3922, Val Loss: 0.4634\n",
      "Epoch [42/100], Loss: 0.3914, Val Loss: 0.4651\n",
      "Epoch [43/100], Loss: 0.3904, Val Loss: 0.4689\n",
      "Epoch [44/100], Loss: 0.3890, Val Loss: 0.4679\n",
      "Epoch [45/100], Loss: 0.3880, Val Loss: 0.4682\n",
      "Epoch [46/100], Loss: 0.3871, Val Loss: 0.4775\n",
      "Epoch [47/100], Loss: 0.3861, Val Loss: 0.4697\n",
      "Epoch [48/100], Loss: 0.3847, Val Loss: 0.4757\n",
      "Epoch [49/100], Loss: 0.3839, Val Loss: 0.4729\n",
      "Epoch [50/100], Loss: 0.3828, Val Loss: 0.4745\n",
      "Epoch [51/100], Loss: 0.3809, Val Loss: 0.4808\n",
      "Epoch [52/100], Loss: 0.3799, Val Loss: 0.4781\n",
      "Epoch [53/100], Loss: 0.3790, Val Loss: 0.4781\n",
      "Epoch [54/100], Loss: 0.3777, Val Loss: 0.4844\n",
      "Epoch [55/100], Loss: 0.3764, Val Loss: 0.4853\n",
      "Epoch [56/100], Loss: 0.3763, Val Loss: 0.4932\n",
      "Epoch [57/100], Loss: 0.3739, Val Loss: 0.4838\n",
      "Epoch [58/100], Loss: 0.3727, Val Loss: 0.4790\n",
      "Epoch [59/100], Loss: 0.3713, Val Loss: 0.4924\n",
      "Epoch [60/100], Loss: 0.3691, Val Loss: 0.4872\n",
      "Epoch [61/100], Loss: 0.3636, Val Loss: 0.4987\n",
      "Epoch [62/100], Loss: 0.3622, Val Loss: 0.4987\n",
      "Epoch [63/100], Loss: 0.3610, Val Loss: 0.5027\n",
      "Epoch [64/100], Loss: 0.3607, Val Loss: 0.5038\n",
      "Epoch [65/100], Loss: 0.3598, Val Loss: 0.5087\n",
      "Epoch [66/100], Loss: 0.3593, Val Loss: 0.5088\n",
      "Epoch [67/100], Loss: 0.3584, Val Loss: 0.5062\n",
      "Epoch [68/100], Loss: 0.3582, Val Loss: 0.5103\n",
      "Epoch [69/100], Loss: 0.3579, Val Loss: 0.5112\n",
      "Epoch [70/100], Loss: 0.3577, Val Loss: 0.5091\n",
      "Epoch [71/100], Loss: 0.3567, Val Loss: 0.5094\n",
      "Epoch [72/100], Loss: 0.3562, Val Loss: 0.5084\n",
      "Epoch [73/100], Loss: 0.3560, Val Loss: 0.5115\n",
      "Epoch [74/100], Loss: 0.3558, Val Loss: 0.5139\n",
      "Epoch [75/100], Loss: 0.3548, Val Loss: 0.5120\n",
      "Epoch [76/100], Loss: 0.3547, Val Loss: 0.5129\n",
      "Epoch [77/100], Loss: 0.3546, Val Loss: 0.5146\n",
      "Epoch [78/100], Loss: 0.3539, Val Loss: 0.5164\n",
      "Epoch [79/100], Loss: 0.3538, Val Loss: 0.5175\n",
      "Epoch [80/100], Loss: 0.3530, Val Loss: 0.5182\n",
      "Epoch [81/100], Loss: 0.3532, Val Loss: 0.5182\n",
      "Epoch [82/100], Loss: 0.3524, Val Loss: 0.5150\n",
      "Epoch [83/100], Loss: 0.3516, Val Loss: 0.5205\n",
      "Epoch [84/100], Loss: 0.3518, Val Loss: 0.5235\n",
      "Epoch [85/100], Loss: 0.3511, Val Loss: 0.5223\n",
      "Epoch [86/100], Loss: 0.3508, Val Loss: 0.5199\n",
      "Epoch [87/100], Loss: 0.3506, Val Loss: 0.5134\n",
      "Epoch [88/100], Loss: 0.3497, Val Loss: 0.5191\n",
      "Epoch [89/100], Loss: 0.3495, Val Loss: 0.5279\n",
      "Epoch [90/100], Loss: 0.3493, Val Loss: 0.5226\n",
      "Epoch [91/100], Loss: 0.3477, Val Loss: 0.5254\n",
      "Epoch [92/100], Loss: 0.3473, Val Loss: 0.5260\n",
      "Epoch [93/100], Loss: 0.3475, Val Loss: 0.5253\n",
      "Epoch [94/100], Loss: 0.3477, Val Loss: 0.5258\n",
      "Epoch [95/100], Loss: 0.3474, Val Loss: 0.5257\n",
      "Epoch [96/100], Loss: 0.3473, Val Loss: 0.5257\n",
      "Epoch [97/100], Loss: 0.3477, Val Loss: 0.5264\n",
      "Epoch [98/100], Loss: 0.3471, Val Loss: 0.5256\n",
      "Epoch [99/100], Loss: 0.3470, Val Loss: 0.5262\n",
      "Epoch [100/100], Loss: 0.3472, Val Loss: 0.5262\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)\n",
    "num_epochs = 100\n",
    "running_loss = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features.unsqueeze(1))  # Add sequence dimension\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for features, labels in val_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features.unsqueeze(1))\n",
    "            val_loss += criterion(outputs.squeeze(), labels).item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68579d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8290\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in val_loader:\n",
    "        features = features.to(device)\n",
    "        outputs = model(features.unsqueeze(1))\n",
    "        predictions = torch.sigmoid(outputs.squeeze()).cpu().numpy()\n",
    "        accuracies.append((predictions > 0.5) == labels.numpy())\n",
    "\n",
    "print(f\"Validation Accuracy: {np.concat(accuracies, axis=0).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb7311f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"transformer_classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
